# üöÄ GU√çA DE MEJORA PARA AGENTE RAG

## üéØ TU PROBLEMA ACTUAL

> "Recupera documentos pero no responde bien algunas preguntas"

Esto significa:
- ‚úÖ El retrieval funciona (encuentra documentos)
- ‚ùå Pero el LLM no usa bien el contexto o el contexto no es el correcto

## üìä M√âTRICAS CLAVE PARA RAG

### 1. **Retrieval Metrics** (Calidad de b√∫squeda)

#### a) Similarity Score
- **Qu√© es:** Qu√© tan similar es el chunk recuperado a la pregunta
- **Rango:** 0.0 - 1.0 (mayor = mejor)
- **Meta:** > 0.5 para preguntas espec√≠ficas
- **C√≥mo medirlo:**
```bash
python template/scripts/evaluate_retrieval.py --instance instances/beca18 --query "tu pregunta"
```

#### b) Precision@K
- **Qu√© es:** De los K chunks recuperados, cu√°ntos son relevantes
- **Meta:** > 0.6 (60% o m√°s relevantes)

#### c) Recall
- **Qu√© es:** Si recuperas toda la informaci√≥n necesaria
- **Meta:** > 0.8 (no dejar informaci√≥n importante fuera)

### 2. **Generation Metrics** (Calidad de respuesta)

#### a) Fidelidad (Faithfulness)
- **Qu√© es:** La respuesta se basa solo en el contexto (sin inventar)
- **Meta:** > 0.9 (90% de informaci√≥n del contexto)

#### b) Relevancia (Relevance)
- **Qu√© es:** La respuesta contesta la pregunta
- **Meta:** > 0.8

#### c) Completitud (Completeness)
- **Qu√© es:** La respuesta es completa, no parcial
- **Meta:** > 0.7

### 3. **System Metrics** (Rendimiento)

- **Latencia:** < 5 segundos por query
- **Costo:** $0 (todo local üòé)
- **Throughput:** Queries por minuto

## üîß C√ìMO MEJORAR TU AGENTE

### Fase 1: DIAGN√ìSTICO

```bash
# 1. Evaluar qu√© chunks recupera
python template/scripts/evaluate_retrieval.py --instance instances/beca18

# 2. Analizar configuraci√≥n actual
python template/scripts/tune_parameters.py --instance instances/beca18 --analyze

# 3. Probar diferentes configuraciones
python template/scripts/tune_parameters.py --instance instances/beca18 --query "tu pregunta problem√°tica"
```

### Fase 2: AJUSTAR CHUNKING

Edita `instances/beca18/config/agent_config.yaml`:

```yaml
preprocessing:
  # CONFIGURACI√ìN ACTUAL
  chunk_size: 800        # ‚Üê PRUEBA: 1000-1200 para m√°s contexto
  chunk_overlap: 100     # ‚Üê PRUEBA: 150-200 (15-20% del chunk_size)
  min_chunk_size: 100    # Est√° bien
```

**Despu√©s de cambiar, RE-VECTORIZAR:**
```bash
python template/scripts/vectorize.py --instance instances/beca18
```

### Fase 3: MEJORAR EL PROMPT

#### Prompt Actual (instances/beca18/prompts/system_prompt.txt)

**MEJORAS SUGERIDAS:**

```plaintext
Eres un asistente especializado en Beca 18, el programa de becas del gobierno peruano administrado por Pronabec.

Tu funci√≥n es ayudar a estudiantes y postulantes a entender:
- Requisitos de postulaci√≥n
- Proceso de inscripci√≥n  
- Modalidades de beca disponibles
- Beneficios y coberturas
- Cronogramas y fechas importantes
- Obligaciones de los becarios

INSTRUCCIONES CR√çTICAS:
1. Lee CUIDADOSAMENTE todo el contexto proporcionado antes de responder
2. Responde √öNICAMENTE usando informaci√≥n expl√≠cita del contexto
3. Si la informaci√≥n est√° en el contexto pero no es clara, di "La informaci√≥n est√° presente pero necesita interpretaci√≥n"
4. Si NO est√° en el contexto, di exactamente: "No encontr√© esa informaci√≥n en los documentos de Beca 18"
5. NO asumas, NO infiereas, NO completes informaci√≥n faltante
6. Cita el documento y secci√≥n cuando sea posible (ej: "Seg√∫n el art√≠culo 6.1...")
7. Si hay m√∫ltiples respuestas posibles, menci√≥nalas todas

FORMATO DE RESPUESTA:
- S√© espec√≠fico y directo
- Usa bullet points para listas
- Menciona n√∫meros de art√≠culo/secci√≥n cuando aplique
- Usa un tono profesional pero amigable

CONTEXTO DE LOS DOCUMENTOS:
{context}

Responde la pregunta del usuario bas√°ndote EXCLUSIVAMENTE en el contexto anterior.
```

### Fase 4: AJUSTAR PAR√ÅMETROS DE QUERY

En tu c√≥digo o al consultar:

```python
# PARA PREGUNTAS ESPEC√çFICAS (ej: "¬øCu√°l es el art√≠culo 6.1?")
response = agent.query(
    question=pregunta,
    n_results=3,          # Pocos chunks, m√°s precisi√≥n
    temperature=0.1       # Muy determinista
)

# PARA PREGUNTAS GENERALES (ej: "¬øC√≥mo funciona Beca 18?")
response = agent.query(
    question=pregunta,
    n_results=8,          # M√°s chunks, m√°s contexto
    temperature=0.3       # Un poco m√°s flexible
)

# PARA PREGUNTAS COMPLEJAS (ej: "Compara las modalidades...")
response = agent.query(
    question=pregunta,
    n_results=15,         # Muchos chunks
    temperature=0.2       # Balance
)
```

### Fase 5: MEJORAR MODELO LLM

Si `qwen2.5:7b` no funciona bien:

```bash
# Opci√≥n 1: Modelo m√°s grande (mejor comprensi√≥n)
ollama pull qwen2.5:14b

# Opci√≥n 2: Mejor contexto largo
ollama pull llama3.1:8b

# Opci√≥n 3: M√°s r√°pido pero bueno
ollama pull mistral:7b
```

Cambiar en `agent_config.yaml`:
```yaml
llm:
  model: "llama3.1:8b"  # ‚Üê Cambiar aqu√≠
  temperature: 0.2       # ‚Üê Reducir para m√°s precisi√≥n
  max_tokens: 2000       # ‚Üê Aumentar para respuestas completas
```

### Fase 6: REFORMULAR PREGUNTAS

**Mal ‚ùå**
```
"Dame info sobre requisitos"
```

**Bien ‚úÖ**
```
"¬øCu√°les son los requisitos de postulaci√≥n para Beca 18 seg√∫n las bases?"
```

**Por qu√©:** Preguntas espec√≠ficas ‚Üí mejor retrieval ‚Üí mejores respuestas

## üß™ EXPERIMENTOS RECOMENDADOS

### Experimento 1: Chunk Size
```bash
# Probar con chunks m√°s grandes
# Editar agent_config.yaml: chunk_size: 1200
python template/scripts/vectorize.py --instance instances/beca18
python template/scripts/query_agent.py --instance instances/beca18
# ¬øMejora? ‚Üí Mantener. ¬øEmpeora? ‚Üí Revertir
```

### Experimento 2: N_results
```bash
# Probar recuperando m√°s chunks
python template/scripts/tune_parameters.py \
  --instance instances/beca18 \
  --query "pregunta que falla"
# Ver cu√°l configuraci√≥n funciona mejor
```

### Experimento 3: Modelo LLM
```bash
# Probar diferentes modelos
ollama pull llama3.1:8b
# Cambiar en config y probar
```

## üìà CHECKLIST DE OPTIMIZACI√ìN

### Nivel 1: B√°sico (haz esto YA)
- [ ] Ejecutar `evaluate_retrieval.py` para ver scores
- [ ] Ejecutar `tune_parameters.py --analyze` 
- [ ] Verificar que chunk_size est√© entre 800-1200
- [ ] Verificar que temperature sea <= 0.3

### Nivel 2: Intermedio
- [ ] Aumentar chunk_overlap a 15-20% del chunk_size
- [ ] Re-vectorizar con nueva configuraci√≥n
- [ ] Mejorar system_prompt con instrucciones m√°s espec√≠ficas
- [ ] Probar n_results entre 5-10 en lugar de 5

### Nivel 3: Avanzado
- [ ] Probar diferentes modelos de embeddings
- [ ] Implementar re-ranking de chunks
- [ ] Agregar filtros de metadata
- [ ] Implementar hybrid search (keyword + vector)

## üéì RECURSOS PARA MEJORAR

### Modelos de embeddings alternativos
```python
# En agent_config.yaml
embeddings:
  # Actual: paraphrase-multilingual-MiniLM-L12-v2
  
  # Alternativa 1: Mejor calidad (m√°s pesado)
  model: "intfloat/multilingual-e5-large"
  
  # Alternativa 2: Espa√±ol espec√≠fico
  model: "hiiamsid/sentence_similarity_spanish_es"
```

### T√©cnicas avanzadas
1. **Hypothetical Document Embeddings (HyDE):** Genera una respuesta hipot√©tica y busca con eso
2. **Query Expansion:** Reformula la pregunta de m√∫ltiples formas
3. **Re-ranking:** Usa un modelo para re-ordenar los chunks recuperados
4. **Parent Document Retrieval:** Recupera chunks peque√±os pero devuelve documentos completos

## üö¶ SE√ëALES DE QUE EST√Å FUNCIONANDO

‚úÖ **BUENO:**
- Similarity scores > 0.5
- Responde preguntas espec√≠ficas correctamente
- Cita fuentes cuando es relevante
- Dice "no s√©" cuando no tiene info

‚ùå **MALO:**
- Similarity scores < 0.3
- Inventa informaci√≥n
- Respuestas gen√©ricas sin usar el contexto
- Siempre dice "no tengo esa informaci√≥n"

## üí° TIPS FINALES

1. **Itera gradualmente:** Cambia una cosa a la vez
2. **Mide antes y despu√©s:** Usa las herramientas de evaluaci√≥n
3. **Documenta qu√© funciona:** Lleva un registro de cambios
4. **Testea con queries reales:** Usa preguntas que har√≠an tus usuarios
5. **Balance:** A veces menos es m√°s (menos chunks puede ser mejor)

---

## üõ†Ô∏è COMANDOS √öTILES

```bash
# Ver qu√© recupera una query
python template/scripts/evaluate_retrieval.py --instance instances/beca18

# Analizar configuraci√≥n
python template/scripts/tune_parameters.py --instance instances/beca18 --analyze

# Probar configuraciones
python template/scripts/tune_parameters.py --instance instances/beca18 --query "tu pregunta"

# Re-vectorizar despu√©s de cambios
python template/scripts/vectorize.py --instance instances/beca18

# Consultar
python template/scripts/query_agent.py --instance instances/beca18
```
