# Local-RAG-Agent

Sistema RAG (Retrieval-Augmented Generation) modular para crear agentes especializados con conocimiento privado local.

## CaracterÃ­sticas

- ğŸ”’ 100% local y privado
- ğŸ¤– IntegraciÃ³n con Ollama (LLMs locales)
- ğŸ“š VectorizaciÃ³n de documentos (PDF, DOCX, TXT, Excel)
- ğŸ¢ Multi-instancia (mÃºltiples agentes especializados)
- âš¡ ChromaDB para bÃºsqueda vectorial eficiente
- ğŸ¯ Arquitectura modular

## Requisitos

- Python 3.10+
- Ollama instalado
- ~10GB RAM (mÃ¡s si usas modelos grandes)

## InstalaciÃ³n

### 1. Clonar e instalar dependencias

```bash
git clone <tu-repo>
cd Local-RAG-Agent

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### 2. Instalar Ollama y descargar modelo

```bash
# Instalar Ollama desde: https://ollama.com/download

# Descargar modelo
ollama pull qwen2.5:7b
```

## Uso RÃ¡pido

### 1. Preparar documentos

Coloca tus PDFs en la carpeta de la instancia:

```bash
cp mi_documento.pdf instances/beca18/data/raw/
```

### 2. Vectorizar documentos

```bash
python template/scripts/vectorize.py --instance instances/beca18
```

### 3. Consultar al agente

**Modo interactivo:**
```bash
python template/scripts/query_agent.py --instance instances/beca18
```

**Consulta Ãºnica:**
```bash
python template/scripts/query_agent.py \
  --instance instances/beca18 \
  --query "Â¿CuÃ¡les son los requisitos?"
```

## Estructura del Proyecto

```
Local-RAG-Agent/
â”œâ”€â”€ template/              # CÃ³digo base
â”‚   â”œâ”€â”€ core/             # Servicios principales
â”‚   â”œâ”€â”€ preprocessing/    # Procesamiento de documentos
â”‚   â”œâ”€â”€ scripts/          # Scripts CLI
â”‚   â””â”€â”€ utils/            # Utilidades
â”‚
â”œâ”€â”€ instances/            # Instancias de agentes
â”‚   â””â”€â”€ beca18/
â”‚       â”œâ”€â”€ config/       # ConfiguraciÃ³n
â”‚       â”œâ”€â”€ data/
â”‚       â”‚   â”œâ”€â”€ raw/      # PDFs originales
â”‚       â”‚   â””â”€â”€ vectordb/ # Base vectorial (generada)
â”‚       â””â”€â”€ prompts/      # Prompts personalizados
â”‚
â””â”€â”€ requirements.txt
```

## Crear Nueva Instancia

```bash
# Copiar estructura
cp -r instances/beca18 instances/mi_agente

# Editar configuraciÃ³n
nano instances/mi_agente/config/agent_config.yaml

# Personalizar prompts
nano instances/mi_agente/prompts/system_prompt.txt

# Agregar documentos
cp docs/* instances/mi_agente/data/raw/

# Vectorizar
python template/scripts/vectorize.py --instance instances/mi_agente
```

## ConfiguraciÃ³n

Edita `instances/<tu_instancia>/config/agent_config.yaml`:

```yaml
agent:
  name: "Mi Agente"
  description: "DescripciÃ³n"

llm:
  model: "qwen2.5:7b"
  temperature: 0.3

embeddings:
  model: "paraphrase-multilingual-MiniLM-L12-v2"
  device: "cpu"

preprocessing:
  chunk_size: 800
  chunk_overlap: 100
```

## Modelos Recomendados

### LLMs (Ollama)
- `qwen2.5:7b` - Excelente espaÃ±ol, recomendado
- `llama3.1:8b` - Bueno general, contexto largo
- `mistral:7b` - RÃ¡pido

### Embeddings
- `paraphrase-multilingual-MiniLM-L12-v2` - Default, multilenguaje
- `intfloat/multilingual-e5-large` - Mayor calidad, mÃ¡s pesado

## SoluciÃ³n de Problemas

**Error: "Modelo no encontrado"**
```bash
ollama pull qwen2.5:7b
```

**Error: "No se puede conectar con Ollama"**
```bash
# Verificar que Ollama estÃ¡ corriendo
ollama list
```

**Base vectorial vacÃ­a**
```bash
# Verificar que tienes PDFs en data/raw/
ls instances/beca18/data/raw/

# Re-vectorizar
python template/scripts/vectorize.py --instance instances/beca18
```

## Licencia

MIT

## Contribuciones

Â¡Contribuciones bienvenidas! Abre un issue o PR.